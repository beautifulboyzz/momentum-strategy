import streamlit as st
import pandas as pd
import numpy as np
import os
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta

# ================= 1. ç³»ç»Ÿé…ç½® =================
st.set_page_config(page_title="Dual Momentumå›æµ‹ç³»ç»Ÿ", layout="wide", page_icon="âš¡")

# --- è·¯å¾„è‡ªåŠ¨é€‚é…é€»è¾‘ (ä¿®æ”¹éƒ¨åˆ†) ---
# 1. å®šä¹‰æœ¬åœ°ç»å¯¹è·¯å¾„ (ä½ çš„ç”µè„‘è°ƒè¯•ç”¨)
local_absolute_path = r"D:\SARæ—¥é¢‘\å…¨éƒ¨å“ç§æ—¥çº¿"

# 2. è‡ªåŠ¨åˆ¤æ–­ç¯å¢ƒ
if os.path.exists(local_absolute_path):
    # å¦‚æœæœ¬åœ°è·¯å¾„å­˜åœ¨ï¼Œè¯´æ˜åœ¨ä½ çš„ç”µè„‘ä¸Š
    DEFAULT_DATA_FOLDER = local_absolute_path
else:
    # å¦åˆ™è¯´æ˜åœ¨ Streamlit äº‘ç«¯ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„ 'data'
    # æ³¨æ„ï¼šä½ éœ€è¦æŠŠ csv æ–‡ä»¶æ”¾å…¥é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ data æ–‡ä»¶å¤¹ä¸­
    DEFAULT_DATA_FOLDER = "data"


# ================= 2. æ•°æ®å¤„ç† =================

def read_robust_csv(f):
    """
    é€šç”¨CSVè¯»å–å‡½æ•° (æ”¯æŒ gbk å’Œ utf-8)
    """
    for enc in ['gbk', 'utf-8']:
        try:
            df = pd.read_csv(f, encoding=enc)
            cols = [str(c).strip() for c in df.columns]
            rename_map = {}

            # æ¨¡ç³ŠåŒ¹é…åˆ—å
            for c in df.columns:
                c_str = str(c).strip()
                if c_str in ['æ—¥æœŸ', 'æ—¥æœŸ/æ—¶é—´', 'date', 'Date']: rename_map[c] = 'date'
                if c_str in ['æ”¶ç›˜ä»·', 'æ”¶ç›˜', 'close', 'price', 'Close']: rename_map[c] = 'close'
                if c_str in ['æœ€é«˜ä»·', 'æœ€é«˜', 'high', 'High']: rename_map[c] = 'high'
                if c_str in ['æœ€ä½ä»·', 'æœ€ä½', 'low', 'Low']: rename_map[c] = 'low'

            df.rename(columns=rename_map, inplace=True)

            if 'date' in df.columns and 'close' in df.columns:
                return df
        except:
            continue
    return None


@st.cache_data(ttl=3600)
def load_data_and_calc_atr(folder, atr_window=20):
    """
    è¯»å–æ•°æ® (å« ATR è®¡ç®—å’Œ Low ä»·æ ¼è¯»å–)
    """
    # è·¯å¾„æ£€æŸ¥
    if not os.path.exists(folder):
        return None, None, None, f"è·¯å¾„ä¸å­˜åœ¨: {folder} (è¯·ç¡®ä¿åœ¨GitHubä¸Šä¼ äº†dataæ–‡ä»¶å¤¹)"

    files = [f for f in os.listdir(folder) if f.endswith('.csv')]
    if not files:
        return None, None, None, f"åœ¨ {folder} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶"

    price_dict = {}
    vol_dict = {}
    low_dict = {}  # å­˜å‚¨æœ€ä½ä»·ç”¨äºæ­¢æŸ

    progress_bar = st.progress(0, text="æ­£åœ¨åŠ è½½æ•°æ®...")

    for i, file in enumerate(files):
        # å‰”é™¤é€»è¾‘
        if "çº¤ç»´æ¿" in file or "èƒ¶åˆæ¿" in file or "çº¿æ" in file:
            continue

        name = file.split('.')[0].replace("ä¸»è¿", "").replace("æ—¥çº¿", "")
        path = os.path.join(folder, file)

        df = read_robust_csv(path)
        if df is None: continue

        try:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            df.dropna(subset=['date', 'close', 'high', 'low'], inplace=True)
            df['date'] = df['date'].dt.normalize()
            df.sort_values('date', inplace=True)
            df.set_index('date', inplace=True)

            # å»é‡
            df = df[~df.index.duplicated(keep='last')]

            # --- è®¡ç®— ATR/NATR ---
            prev_close = df['close'].shift(1)
            tr1 = df['high'] - df['low']
            tr2 = (df['high'] - prev_close).abs()
            tr3 = (df['low'] - prev_close).abs()
            tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

            atr = tr.rolling(atr_window).mean()
            natr = atr / df['close']

            price_dict[name] = df['close']
            vol_dict[name] = natr
            low_dict[name] = df['low']

        except Exception as e:
            continue

        if i % 10 == 0:
            progress_bar.progress((i + 1) / len(files), text=f"åŠ è½½: {name}")

    progress_bar.empty()

    if not price_dict:
        return None, None, None, "æœªè¯»å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥CSVæ ¼å¼"

    # åˆå¹¶ä¸ºå®½è¡¨
    df_prices = pd.DataFrame(price_dict).sort_index().ffill()
    df_vols = pd.DataFrame(vol_dict).sort_index().ffill()
    df_lows = pd.DataFrame(low_dict).sort_index().ffill()

    return df_prices, df_vols, df_lows, None


# ================= 3. æ ¸å¿ƒç­–ç•¥é€»è¾‘ =================

def run_strategy_logic(df_prices, df_vols, df_lows, params):
    """
    æ ¸å¿ƒç­–ç•¥é€»è¾‘ï¼šåŠ¨é‡è¯„åˆ† + å‡çº¿è¿‡æ»¤ + æ³¢åŠ¨ç‡åŠ æƒ + æ—¥å†…æ­¢æŸ
    """
    # è§£åŒ…å‚æ•°
    lookback_short = params['short']
    lookback_long = params['long']
    hold_num = params['hold_num']
    filter_ma = params['ma']
    stop_loss_pct = params['stop_loss_pct']

    start_date = pd.to_datetime(params['start_date'])
    end_date = pd.to_datetime(params['end_date'])

    # --- A. å› å­è®¡ç®— ---
    mom_short = df_prices.pct_change(lookback_short)
    mom_long = df_prices.pct_change(lookback_long)
    # æ ¸å¿ƒå…¬å¼ï¼š0.4 * Short + 0.6 * Long
    momentum_score = 0.4 * mom_short + 0.6 * mom_long
    ma_filter = df_prices > df_prices.rolling(filter_ma).mean()
    asset_daily_rets = df_prices.pct_change().fillna(0)

    # --- B. åˆå§‹åŒ–å›æµ‹å˜é‡ ---
    capital = 1.0
    nav_record = []
    asset_contribution = {}
    logs = []

    # æˆªå–æ—¶é—´æ®µ
    full_dates = df_prices.index
    try:
        start_idx_loc = full_dates.get_indexer([start_date], method='bfill')[0]
    except:
        start_idx_loc = 0

    min_idx = max(lookback_long, filter_ma, 20)
    start_idx_loc = max(start_idx_loc, min_idx)

    if start_idx_loc >= len(full_dates):
        return pd.DataFrame(), pd.DataFrame(), ["é€‰å®šæ—¶é—´å†…æ•°æ®ä¸è¶³"]

    # è¿è¡Œæ—¶çŠ¶æ€å˜é‡
    weights = {}  # ç›®æ ‡æŒä»“æƒé‡
    curr_holdings = {}  # å®é™…æŒä»“æƒé‡
    entry_prices = {}  # å‚è€ƒä»·

    # æ—¥å¿—ç¼“å­˜
    log_buffer_pnl = []
    cycle_count = 1
    log_start_date = full_dates[start_idx_loc]

    # --- C. æŒ‰æ—¥å¾ªç¯ ---
    for i in range(start_idx_loc, len(full_dates)):
        curr_date = full_dates[i]
        if curr_date > end_date: break
        prev_date = full_dates[i - 1]

        # 1. æ¯æ—¥é€‰è‚¡ (Daily Rebalance)
        try:
            scores = momentum_score.loc[prev_date].dropna()
            vols = df_vols.loc[prev_date]

            if len(scores) < hold_num:
                weights = {}
            else:
                top = scores.sort_values(ascending=False).head(hold_num).index.tolist()
                valid = [a for a in top if ma_filter.loc[prev_date, a]]

                if not valid:
                    weights = {}
                else:
                    # é£é™©å¹³ä»· (Risk Parity based on Volatility)
                    sub_vols = vols[valid]
                    inv = 1.0 / (sub_vols + 1e-6)
                    weights = (inv / inv.sum()).to_dict()

            entry_prices = {a: df_prices.loc[prev_date, a] for a in weights.keys()}
            curr_holdings = weights.copy()

        except KeyError:
            weights = {}
            curr_holdings = {}

        # 2. æ—¥å†…é£æ§ä¸æ”¶ç›Šç»“ç®—
        daily_pnl = 0.0
        stopped_assets = []

        for asset, w in list(curr_holdings.items()):
            if w == 0: continue

            # æ£€æŸ¥æ­¢æŸ
            today_low = df_lows.loc[curr_date, asset]
            ref_price = entry_prices.get(asset, df_prices.loc[curr_date, asset])

            # å¦‚æœæœ€ä½ä»·è§¦å‘æ­¢æŸçº¿
            if ref_price > 0 and (today_low / ref_price - 1) < -stop_loss_pct:
                # è§¦å‘æ­¢æŸï¼ŒæŒ‰æ­¢æŸå¹…åº¦ç»“ç®—
                actual_ret = -stop_loss_pct
                daily_pnl += w * actual_ret
                asset_contribution[asset] = asset_contribution.get(asset, 0.0) + w * actual_ret

                curr_holdings[asset] = 0  # æ ‡è®°ä¸ºå¹³ä»“
                stopped_assets.append(asset)
            else:
                # æ­£å¸¸æŒæœ‰
                ret = asset_daily_rets.loc[curr_date, asset]
                daily_pnl += w * ret
                asset_contribution[asset] = asset_contribution.get(asset, 0.0) + w * ret

        capital *= (1 + daily_pnl)
        nav_record.append({'date': curr_date, 'nav': capital})

        # --- D. ç”Ÿæˆæ—¥å¿— ---
        log_buffer_pnl.append(daily_pnl)

        if stopped_assets:
            logs.append(
                f"âš ï¸ [{curr_date.strftime('%Y-%m-%d')}] è§¦å‘æ­¢æŸ: {', '.join(stopped_assets)} (æŒ‰ {-stop_loss_pct * 100}% ç¦»åœº)")

        # æ¯5å¤©æˆ–æœ€åä¸€å¤©èšåˆæ—¥å¿—
        if len(log_buffer_pnl) == 5 or i == len(full_dates) - 1 or curr_date == end_date:
            cycle_ret = (np.prod([1 + r for r in log_buffer_pnl]) - 1)
            hold_str = ", ".join([f"{a}({w:.1%})" for a, w in curr_holdings.items() if w > 0])
            if not hold_str: hold_str = "ç©ºä»“"

            end_d_str = curr_date.strftime('%Y-%m-%d')
            start_d_str = log_start_date.strftime('%Y-%m-%d')

            log_chunk = f"Cycle {cycle_count:02d} ({start_d_str} ~ {end_d_str}) | æ”¶ç›Š: {cycle_ret * 100:>+5.1f}% | å‡€å€¼: {capital:.4f}\n"
            log_chunk += f"   >> æŒä»“: {hold_str}\n"
            log_chunk += "-" * 60

            logs.append(log_chunk)
            log_buffer_pnl = []
            cycle_count += 1
            if i < len(full_dates) - 1:
                log_start_date = full_dates[i + 1]

    return pd.DataFrame(nav_record), pd.DataFrame(list(asset_contribution.items()),
                                                  columns=['Asset', 'Contribution']), logs


# ================= 4. UI é¡µé¢ =================

with st.sidebar:
    st.header("åŒé‡åŠ¨é‡é…ç½®")

    # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ•°æ®è·¯å¾„ (åªè¯»)
    st.info(f"å½“å‰æ•°æ®æº: `{DEFAULT_DATA_FOLDER}`")

    # ä¾ç„¶ä¿ç•™è¾“å…¥æ¡†ï¼Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨æ”¹ (å¯é€‰)
    data_folder = st.text_input("æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„", value=DEFAULT_DATA_FOLDER)
    st.divider()

    st.subheader("ğŸ—“ï¸ æ ¸å¿ƒå‚æ•°")
    col_d1, col_d2 = st.columns(2)
    start_d_input = col_d1.date_input("å¼€å§‹æ—¥æœŸ", value=pd.to_datetime("2024-01-01"))
    end_d_input = col_d2.date_input("ç»“æŸæ—¥æœŸ", value=pd.to_datetime("2025-12-31"))

    hold_num_input = st.number_input("æŒä»“æ•°é‡", min_value=1, max_value=20, value=5)
    stop_loss_pct = st.number_input("å•æ—¥ä¸ªè‚¡æ­¢æŸ (%)", min_value=0.0, max_value=20.0, value=4.0, step=0.5) / 100.0

    with st.expander("ğŸ› ï¸ ç®—æ³•å‚æ•° (5/20)"):
        lookback_short = st.number_input("çŸ­æœŸåŠ¨é‡ (Short)", value=5)
        lookback_long = st.number_input("é•¿æœŸåŠ¨é‡ (Long)", value=20)
        filter_ma = st.number_input("å‡çº¿è¿‡æ»¤ (MA)", value=60)
        atr_window = st.number_input("ATRå‘¨æœŸ", value=20)

    run_btn = st.button(" è¿è¡Œç­–ç•¥", type="primary", use_container_width=True)

# ä¸»ç•Œé¢
st.title("Dual Momentum ç­–ç•¥å›æµ‹")

if run_btn:
    with st.spinner('æ­£åœ¨åŠ è½½æ•°æ® (å«æœ€ä½ä»·æ£€æŸ¥)...'):
        # ä½¿ç”¨ä¾§è¾¹æ æœ€ç»ˆç¡®è®¤çš„è·¯å¾„
        df_prices, df_vols, df_lows, err = load_data_and_calc_atr(data_folder, atr_window)

    if err:
        st.error(err)
        if "è·¯å¾„ä¸å­˜åœ¨" in err and "data" in err:
            st.warning("æç¤º: å¦‚æœæ˜¯åœ¨äº‘ç«¯è¿è¡Œï¼Œè¯·ç¡®ä¿ä½ å·²ç»å°†csvæ–‡ä»¶ä¸Šä¼ åˆ°äº†GitHubä»“åº“çš„ 'data' æ–‡ä»¶å¤¹ä¸­ã€‚")
    else:
        params = {
            'short': lookback_short,
            'long': lookback_long,
            'ma': filter_ma,
            'hold_num': hold_num_input,
            'stop_loss_pct': stop_loss_pct,
            'start_date': start_d_input,
            'end_date': end_d_input
        }

        with st.spinner('æ­£åœ¨é€æ—¥æ¨¡æ‹Ÿ (å«æ—¥å†…æ­¢æŸé€»è¾‘)...'):
            res_nav, res_contrib, res_logs = run_strategy_logic(df_prices, df_vols, df_lows, params)

        if res_nav.empty:
            st.warning("è¯¥æ—¶é—´æ®µå†…æ— äº¤æ˜“æ•°æ®æˆ–æ•°æ®ä¸è¶³ã€‚")
        else:
            # æ•°æ®å¤„ç†
            res_nav.set_index('date', inplace=True)
            res_contrib.sort_values('Contribution', ascending=False, inplace=True)

            # æŒ‡æ ‡è®¡ç®—
            total_ret = res_nav['nav'].iloc[-1] - 1
            days = (res_nav.index[-1] - res_nav.index[0]).days
            annual_ret = (1 + total_ret) ** (365 / days) - 1 if days > 0 else 0

            running_max = res_nav['nav'].cummax()
            dd = (res_nav['nav'] - running_max) / running_max
            max_dd = dd.min()

            daily_rets = res_nav['nav'].pct_change().fillna(0)
            sharpe = (daily_rets.mean() * 252) / (daily_rets.std() * np.sqrt(252)) if daily_rets.std() > 0 else 0

            st.success("å›æµ‹å®Œæˆï¼")

            # æŒ‡æ ‡å¡ç‰‡
            k1, k2, k3, k4 = st.columns(4)
            k1.metric("æ€»æ”¶ç›Šç‡", f"{total_ret * 100:.2f}%", delta_color="normal")
            k2.metric("å¹´åŒ–æ”¶ç›Š (CAGR)", f"{annual_ret * 100:.2f}%")
            k3.metric("æœ€å¤§å›æ’¤", f"{max_dd * 100:.2f}%", delta_color="inverse")
            k4.metric("å¤æ™®æ¯”ç‡", f"{sharpe:.2f}")

            # å›¾è¡¨ Tabs
            tab_chart, tab_attr, tab_log = st.tabs(["ğŸ“ˆ èµ„é‡‘æ›²çº¿", "ğŸ† æ”¶ç›Šå½’å› ", "ğŸ“ äº¤æ˜“æ—¥å¿—"])

            with tab_chart:
                plot_data = res_nav.copy()
                plot_data['return_pct'] = (plot_data['nav'] - 1) * 100
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=plot_data.index, y=plot_data['return_pct'],
                    mode='lines', name='ç´¯è®¡æ”¶ç›Šç‡',
                    line=dict(color='#ff7f0e', width=2.5),
                    fill='tozeroy', fillcolor='rgba(255, 127, 14, 0.1)'
                ))
                fig.update_layout(
                    title='<b>ç´¯è®¡æ”¶ç›Š</b>', xaxis_title="æ—¥æœŸ", yaxis_title="ç´¯è®¡æ”¶ç›Šç‡ (%)",
                    hovermode="x unified", margin=dict(l=20, r=20, t=60, b=20), plot_bgcolor='white'
                )
                fig.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.05)')
                fig.update_yaxes(showgrid=True, gridcolor='rgba(0,0,0,0.05)', ticksuffix="%")
                st.plotly_chart(fig, use_container_width=True)

            with tab_attr:
                st.markdown("#### å“ç§ç´¯è®¡è´¡çŒ®åº¦")
                res_contrib['Color'] = res_contrib['Contribution'].apply(lambda x: 'red' if x >= 0 else 'green')
                fig_bar = px.bar(res_contrib, x='Contribution', y='Asset', orientation='h',
                                 text_auto='.2%', color='Contribution',
                                 color_continuous_scale=['green', '#f0f2f6', 'red'])
                fig_bar.update_layout(height=max(400, len(res_contrib) * 20),
                                      yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig_bar, use_container_width=True)

                col_t1, col_t2 = st.columns(2)
                with col_t1:
                    st.caption("ğŸ† ç›ˆåˆ©çº¢æ¦œ")
                    st.dataframe(res_contrib.head(5).style.format({"Contribution": "{:.2%}"}), use_container_width=True)
                with col_t2:
                    st.caption("â˜ ï¸ äºæŸé»‘æ¦œ")
                    st.dataframe(
                        res_contrib.tail(5).sort_values("Contribution").style.format({"Contribution": "{:.2%}"}),
                        use_container_width=True)

            with tab_log:
                st.markdown("#### èšåˆäº¤æ˜“æ—¥å¿— (æ¯5å¤© / æ­¢æŸè§¦å‘)")
                log_text = "\n".join(res_logs)
                st.text_area("Log Output", log_text, height=600)

else:
    st.info(f"ğŸ‘ˆ å‡†å¤‡å°±ç»ªï¼Œè¯·ç‚¹å‡»ã€è¿è¡Œç­–ç•¥ã€‘\n\nå½“å‰æ£€æµ‹è·¯å¾„: `{DEFAULT_DATA_FOLDER}`")
    if os.path.exists(data_folder):
        files_count = len([f for f in os.listdir(data_folder) if f.endswith('.csv')])
        st.write(f"ğŸ“‚ ç›®å½•çŠ¶æ€ï¼šæ‰¾åˆ° {files_count} ä¸ªCSVæ–‡ä»¶")
    else:
        st.write("âš ï¸ ç›®å½•çŠ¶æ€ï¼šè·¯å¾„ä¸å­˜åœ¨ (è¯·åœ¨æœ¬åœ°åˆ›å»ºæˆ–åœ¨GitHubä¸Šä¼ dataæ–‡ä»¶å¤¹)")
